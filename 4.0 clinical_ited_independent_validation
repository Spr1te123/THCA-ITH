import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
import os
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.patches as mpatches
from matplotlib.patches import Rectangle

warnings.filterwarnings('ignore')

# è®¾ç½®è¾“å‡ºç›®å½•
OUTPUT_DIR = 'F:/Habitat_radiomics/Publication_Results'
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['font.size'] = 10
plt.rcParams['axes.linewidth'] = 0.5
plt.rcParams['xtick.major.width'] = 0.5
plt.rcParams['ytick.major.width'] = 0.5

# ================== Part 1: æ•°æ®åŠ è½½ ==================

def load_complete_data(dataset_type='zlyy'):
    """åŠ è½½å®Œæ•´æ•°æ®ï¼ŒåŒ…å«æ‰€æœ‰ä¸´åºŠå˜é‡"""
    print(f"\n=== Loading {dataset_type.upper()} Dataset ===")

    # åŠ è½½ä¸´åºŠæ•°æ®
    clinical_file = f'F:/Habitat_radiomics/clinical_features_processed_{dataset_type}.csv'
    clinical_data = pd.read_csv(clinical_file)

    # åŠ è½½iTEDç‰¹å¾
    ited_file = f'F:/Habitat_radiomics/iTED_features_{dataset_type}.csv'
    ited_features = pd.read_csv(ited_file)

    # åŠ è½½Radiomicsç‰¹å¾
    radiomics_file = f'F:/Habitat_radiomics/radiomics_features_{dataset_type}.csv'
    radiomics_features = pd.read_csv(radiomics_file)

    # åŠ è½½3D_ITHscore
    ithscore_file = f'F:/Habitat_radiomics/3D_ITHscore_{dataset_type}.csv'
    ithscore_data = pd.read_csv(ithscore_file)

    # åˆå¹¶æ•°æ®
    data = clinical_data.copy()

    # åˆå¹¶iTEDç‰¹å¾
    ited_cols = [col for col in ited_features.columns if col != 'PatientID']
    data = pd.merge(data, ited_features, on='PatientID', how='inner')

    # åˆå¹¶Radiomicsç‰¹å¾
    radiomics_cols = [col for col in radiomics_features.columns if col != 'PatientID']
    data = pd.merge(data, radiomics_features, on='PatientID', how='inner')

    # åˆå¹¶3D_ITHscore
    data = pd.merge(data, ithscore_data, on='PatientID', how='inner')

    # é‡ç½®ç´¢å¼•
    data = data.reset_index(drop=True)

    # è·å–æ‰€æœ‰ä¸´åºŠå˜é‡ï¼ˆæ’é™¤PatientIDå’Œç»“å±€å˜é‡ï¼‰
    clinical_vars = [col for col in clinical_data.columns
                    if col not in ['PatientID', 'M_stage']]

    print(f"Loaded data shape: {data.shape}")
    print(f"Number of clinical variables: {len(clinical_vars)}")
    print(f"M1 rate: {(data['M_stage'] == 1).mean():.1%}")

    return {
        'full_data': data,
        'clinical_vars': clinical_vars,
        'ited_features': ited_cols,
        'radiomics_features': radiomics_cols,
        'outcome': 'M_stage'
    }

# ================== Part 2: åˆ›å»ºé£é™©åˆ†ç»„ï¼ˆä¿®æ”¹ç‰ˆï¼‰ ==================

def create_risk_groups_from_predictions(data, dataset_type, feature_type):
    """ä»é¢„æµ‹æ–‡ä»¶åˆ›å»ºé£é™©åˆ†ç»„"""
    print(f"\n=== Processing {feature_type} ===")

    # æ ¹æ®ç‰¹å¾ç±»å‹åŠ è½½å¯¹åº”çš„é¢„æµ‹æ–‡ä»¶
    if feature_type == 'iTED':
        pred_file = f'F:/Habitat_radiomics/results/iTED_predictions_all_patients_{dataset_type}.csv'
    elif feature_type == 'Radiomics':
        pred_file = f'F:/Habitat_radiomics/results/Radiomics_predictions_all_patients_{dataset_type}.csv'
    elif feature_type == '3D_ITH':
        # 3D_ITHscoreç›´æ¥ä½¿ç”¨åŸå§‹å€¼
        pred_file = f'F:/Habitat_radiomics/3D_ITHscore_{dataset_type}.csv'
    else:
        raise ValueError(f"Unknown feature type: {feature_type}")

    # è¯»å–é¢„æµ‹æ•°æ®
    pred_data = pd.read_csv(pred_file)

    # è·å–é¢„æµ‹å€¼
    if feature_type in ['iTED', 'Radiomics']:
        # ç¬¬3åˆ—æ˜¯Predicted_Probability
        pred_data = pred_data[['PatientID', 'Predicted_Probability']]
        pred_data.columns = ['PatientID', 'predictions']
    else:  # 3D_ITH
        # ç¬¬2åˆ—æ˜¯3D_ITHscore
        pred_data = pred_data[['PatientID', '3D_ITHscore']]
        pred_data.columns = ['PatientID', 'predictions']

    # åˆå¹¶é¢„æµ‹å€¼åˆ°ä¸»æ•°æ®
    data = pd.merge(data, pred_data, on='PatientID', how='left')

    # åˆ›å»ºå¾—åˆ†åˆ—
    score_col = f'{feature_type}_Score'
    risk_col = f'{feature_type}_Risk'

    data[score_col] = data['predictions']

    # è®¡ç®—æœ€ä¼˜é˜ˆå€¼ï¼ˆä½¿ç”¨æœ‰æ ‡ç­¾çš„æ•°æ®ï¼‰
    y = data['M_stage']
    predictions = data['predictions']

    # ç§»é™¤ç¼ºå¤±å€¼
    mask = ~(predictions.isnull() | y.isnull())
    y_clean = y[mask]
    predictions_clean = predictions[mask]

    if len(y_clean) > 0:
        # è®¡ç®—AUC
        auc = roc_auc_score(y_clean, predictions_clean)

        # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼
        fpr, tpr, thresholds = roc_curve(y_clean, predictions_clean)
        j_scores = tpr - fpr
        optimal_idx = np.argmax(j_scores)
        threshold = thresholds[optimal_idx]

        # åˆ›å»ºé£é™©åˆ†ç»„
        data[risk_col] = 'Low'
        data.loc[data[score_col] >= threshold, risk_col] = 'High'

        print(f"AUC: {auc:.3f}")
        print(f"Threshold: {threshold:.3f}")
        print(f"Risk distribution - Low: {(data[risk_col] == 'Low').sum()}, High: {(data[risk_col] == 'High').sum()}")

    # åˆ é™¤ä¸´æ—¶predictionsåˆ—
    data = data.drop('predictions', axis=1)

    return data

# ================== Part 3: ç»¼åˆå•å› ç´ åˆ†æ ==================

def perform_comprehensive_univariate_analysis(data, clinical_vars):
    """å¯¹æ‰€æœ‰å˜é‡è¿›è¡Œå•å› ç´ åˆ†æ"""

    # å®šä¹‰åˆ†ç±»å˜é‡åŠå…¶å‚è€ƒç±»åˆ«
    categorical_vars_config = {
        'Sex': {'categories': [0, 1], 'labels': ['Female', 'Male'], 'reference': 0},
        'Benign_thyroid_lesions': {'categories': [0, 1], 'labels': ['No', 'Yes'], 'reference': 0},
        'Multifocal': {'categories': [0, 1], 'labels': ['No', 'Yes'], 'reference': 0},
        'Infiltrated_the_adjacent_tissue': {'categories': [0, 1], 'labels': ['No', 'Yes'], 'reference': 0},
        'T_stage': {'categories': ['T1', 'T2', 'T3', 'T4'], 'labels': ['T1', 'T2', 'T3', 'T4'], 'reference': 'T1'},
        'N_stage': {'categories': ['N0', 'N1'], 'labels': ['N0', 'N1'], 'reference': 'N0'},
        'iTED_Risk': {'categories': ['Low', 'High'], 'labels': ['Low', 'High'], 'reference': 'Low'},
        'Radiomics_Risk': {'categories': ['Low', 'High'], 'labels': ['Low', 'High'], 'reference': 'Low'},
        '3D_ITH_Risk': {'categories': ['Low', 'High'], 'labels': ['Low', 'High'], 'reference': 'Low'}
    }

    results = []
    y = data['M_stage']

    # æ‰€æœ‰å¾…åˆ†æå˜é‡
    score_vars = ['iTED_Risk', 'Radiomics_Risk', '3D_ITH_Risk']
    all_vars = score_vars + clinical_vars

    for var in all_vars:
        if var not in data.columns:
            continue

        try:
            # å¤„ç†åˆ†ç±»å˜é‡
            if var in categorical_vars_config:
                config = categorical_vars_config[var]
                var_data = data[var].copy()

                # è·å–å®é™…å­˜åœ¨çš„ç±»åˆ«
                unique_vals = var_data.dropna().unique()

                # å…ˆæ·»åŠ å‚è€ƒç±»åˆ« - ç¡®ä¿æ‰€æœ‰åˆ†ç±»å˜é‡éƒ½æœ‰å‚è€ƒç±»åˆ«
                reference = config['reference']
                ref_label = config['labels'][config['categories'].index(reference)] if reference in config['categories'] else str(reference)

                # æ— è®ºæ•°æ®ä¸­æ˜¯å¦å­˜åœ¨å‚è€ƒç±»åˆ«ï¼Œéƒ½æ·»åŠ åˆ°ç»“æœä¸­
                results.append({
                    'Variable': var,
                    'Category': ref_label,
                    'OR': 1.0,
                    'OR_CI': 'Reference',
                    'P_value': np.nan,
                    'Beta': 0.0,
                    'N': (~var_data.isnull()).sum(),
                    'Events': y[var_data == reference].sum() if reference in unique_vals else 0,
                    'IsReference': True
                })

                # åˆ†æéå‚è€ƒç±»åˆ«
                for val in unique_vals:
                    if val == reference:
                        continue

                    # åˆ›å»ºäºŒåˆ†ç±»å˜é‡
                    X = (var_data == val).astype(float)

                    # ç§»é™¤ç¼ºå¤±å€¼
                    mask = ~(X.isnull() | y.isnull() | var_data.isnull())
                    X_clean = X[mask].values.reshape(-1, 1)
                    y_clean = y[mask].values

                    if len(y_clean) < 20 or y_clean.sum() < 5:
                        continue

                    # é€»è¾‘å›å½’
                    lr = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', max_iter=1000)
                    lr.fit(X_clean, y_clean)

                    # è®¡ç®—ç»Ÿè®¡é‡
                    coef = lr.coef_[0][0]
                    or_val = np.exp(coef)

                    # Bootstrap CI
                    n_bootstrap = 200
                    bootstrap_coefs = []

                    for _ in range(n_bootstrap):
                        idx = np.random.choice(len(y_clean), len(y_clean), replace=True)
                        X_boot = X_clean[idx]
                        y_boot = y_clean[idx]

                        try:
                            lr_boot = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')
                            lr_boot.fit(X_boot, y_boot)
                            bootstrap_coefs.append(lr_boot.coef_[0][0])
                        except:
                            continue

                    if len(bootstrap_coefs) > 50:
                        ci_lower = np.exp(np.percentile(bootstrap_coefs, 2.5))
                        ci_upper = np.exp(np.percentile(bootstrap_coefs, 97.5))

                        se = np.std(bootstrap_coefs)
                        z_score = coef / (se + 1e-8)
                        p_val = 2 * (1 - stats.norm.cdf(np.abs(z_score)))

                        # è·å–ç±»åˆ«æ ‡ç­¾
                        if val in config['categories']:
                            cat_label = config['labels'][config['categories'].index(val)]
                        else:
                            cat_label = str(val)

                        results.append({
                            'Variable': var,
                            'Category': cat_label,
                            'OR': or_val,
                            'OR_CI': f"{or_val:.2f} ({ci_lower:.2f}, {ci_upper:.2f})",
                            'CI_Lower': ci_lower,
                            'CI_Upper': ci_upper,
                            'P_value': p_val,
                            'Beta': coef,
                            'N': len(y_clean),
                            'Events': y_clean.sum(),
                            'IsReference': False
                        })

            else:
                # è¿ç»­å˜é‡
                X = data[[var]]

                # ç§»é™¤ç¼ºå¤±å€¼
                mask = ~(X.isnull().any(axis=1) | y.isnull())
                X_clean = X[mask]
                y_clean = y[mask]

                if len(y_clean) < 20 or y_clean.sum() < 5:
                    continue

                # æ ‡å‡†åŒ–
                X_std = (X_clean - X_clean.mean()) / (X_clean.std() + 1e-8)

                lr = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', max_iter=1000)
                lr.fit(X_std, y_clean)

                # è®¡ç®—ç»Ÿè®¡é‡
                coef = lr.coef_[0][0]
                or_val = np.exp(coef)

                # Bootstrap CI
                n_bootstrap = 200
                bootstrap_coefs = []

                for _ in range(n_bootstrap):
                    idx = np.random.choice(len(y_clean), len(y_clean), replace=True)
                    X_boot = X_std.iloc[idx]
                    y_boot = y_clean.iloc[idx]

                    try:
                        lr_boot = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')
                        lr_boot.fit(X_boot, y_boot)
                        bootstrap_coefs.append(lr_boot.coef_[0][0])
                    except:
                        continue

                if len(bootstrap_coefs) > 50:
                    ci_lower = np.exp(np.percentile(bootstrap_coefs, 2.5))
                    ci_upper = np.exp(np.percentile(bootstrap_coefs, 97.5))

                    se = np.std(bootstrap_coefs)
                    z_score = coef / (se + 1e-8)
                    p_val = 2 * (1 - stats.norm.cdf(np.abs(z_score)))

                    results.append({
                        'Variable': var,
                        'Category': '',
                        'OR': or_val,
                        'OR_CI': f"{or_val:.2f} ({ci_lower:.2f}, {ci_upper:.2f})",
                        'CI_Lower': ci_lower,
                        'CI_Upper': ci_upper,
                        'P_value': p_val,
                        'Beta': coef,
                        'N': len(y_clean),
                        'Events': y_clean.sum(),
                        'IsReference': False
                    })

        except Exception as e:
            print(f"Error analyzing {var}: {str(e)[:100]}")

    return pd.DataFrame(results)

# ================== Part 4: å¤šå› ç´ åˆ†æï¼ˆå…¨é¢ç­–ç•¥ï¼‰ ==================

def perform_multivariate_comprehensive(data, uni_results):
    """ç­–ç•¥2ï¼šä¸‰ä¸ªè¯„åˆ† + æ‰€æœ‰å•å› ç´ æ˜¾è‘—çš„å˜é‡ï¼ˆP<0.1ï¼‰"""

    # è·å–æ˜¾è‘—å˜é‡ï¼ˆP<0.1ï¼Œæ’é™¤å‚è€ƒç±»åˆ«ï¼‰
    significant_df = uni_results[(uni_results['P_value'] < 0.1) & (~uni_results['IsReference'])]
    significant_vars = significant_df['Variable'].unique()

    # ç¡®ä¿ä¸‰ä¸ªè¯„åˆ†éƒ½åŒ…å«
    core_scores = ['iTED_Risk', 'Radiomics_Risk', '3D_ITH_Risk']
    for score in core_scores:
        if score not in significant_vars and score in data.columns:
            significant_vars = np.append(significant_vars, score)

    print(f"\nSignificant variables for multivariate analysis: {list(significant_vars)}")

    # å®šä¹‰åˆ†ç±»å˜é‡é…ç½®
    categorical_vars_config = {
        'Sex': {'categories': [0, 1], 'labels': ['Female', 'Male'], 'reference': 0},
        'Benign_thyroid_lesions': {'categories': [0, 1], 'labels': ['No', 'Yes'], 'reference': 0},
        'Multifocal': {'categories': [0, 1], 'labels': ['No', 'Yes'], 'reference': 0},
        'Infiltrated_the_adjacent_tissue': {'categories': [0, 1], 'labels': ['No', 'Yes'], 'reference': 0},
        'T_stage': {'categories': ['T1', 'T2', 'T3', 'T4'], 'labels': ['T1', 'T2', 'T3', 'T4'], 'reference': 'T1'},
        'N_stage': {'categories': ['N0', 'N1'], 'labels': ['N0', 'N1'], 'reference': 'N0'},
        'iTED_Risk': {'categories': ['Low', 'High'], 'labels': ['Low', 'High'], 'reference': 'Low'},
        'Radiomics_Risk': {'categories': ['Low', 'High'], 'labels': ['Low', 'High'], 'reference': 'Low'},
        '3D_ITH_Risk': {'categories': ['Low', 'High'], 'labels': ['Low', 'High'], 'reference': 'Low'}
    }

    results = []

    try:
        # å‡†å¤‡æ•°æ®
        X_data = pd.DataFrame()
        var_mapping = []

        for var in significant_vars:
            if var not in data.columns:
                continue

            if var in categorical_vars_config:
                config = categorical_vars_config[var]
                var_data = data[var].copy()

                # è·å–å”¯ä¸€å€¼
                unique_vals = var_data.dropna().unique()

                # åˆ›å»ºå“‘å˜é‡ï¼ˆæ’é™¤å‚è€ƒç±»åˆ«ï¼‰
                for val in unique_vals:
                    if val == config['reference']:
                        continue

                    col_name = f"{var}_{val}"
                    X_data[col_name] = (var_data == val).astype(float)

                    # è·å–ç±»åˆ«æ ‡ç­¾
                    if val in config['categories']:
                        cat_label = config['labels'][config['categories'].index(val)]
                    else:
                        cat_label = str(val)

                    var_mapping.append((var, cat_label, col_name))
            else:
                # è¿ç»­å˜é‡
                col_name = f'{var}_std'
                X_data[col_name] = (data[var] - data[var].mean()) / (data[var].std() + 1e-8)
                var_mapping.append((var, '', col_name))

        if X_data.shape[1] == 0:
            return pd.DataFrame()

        y = data['M_stage']

        # ç§»é™¤ç¼ºå¤±å€¼
        mask = ~(X_data.isnull().any(axis=1) | y.isnull())
        X_clean = X_data[mask]
        y_clean = y[mask]

        print(f"\nMultivariate model:")
        print(f"  Variables included: {len(var_mapping)}")
        print(f"  Sample size: {len(y_clean)}, Events: {y_clean.sum()}")

        # é€»è¾‘å›å½’ï¼ˆå¢åŠ æ­£åˆ™åŒ–ï¼‰
        lr = LogisticRegression(penalty='l2', C=0.5, solver='liblinear',
                               class_weight='balanced', max_iter=1000)
        lr.fit(X_clean, y_clean)

        # é¦–å…ˆæ·»åŠ æ‰€æœ‰å‚è€ƒç±»åˆ« - åŒ…æ‹¬æ‰€æœ‰åˆ†ç±»å˜é‡ï¼Œä¸ä»…æ˜¯æ˜¾è‘—çš„
        all_categorical_vars = [v for v in significant_vars if v in categorical_vars_config]

        # ç¡®ä¿åŒ…å«æ‰€æœ‰åœ¨æ•°æ®ä¸­çš„åˆ†ç±»å˜é‡
        for var in categorical_vars_config.keys():
            if var in data.columns and var not in all_categorical_vars:
                all_categorical_vars.append(var)

        for var in all_categorical_vars:
            if var in categorical_vars_config:
                config = categorical_vars_config[var]
                ref_label = config['labels'][config['categories'].index(config['reference'])] \
                           if config['reference'] in config['categories'] else str(config['reference'])

                results.append({
                    'Variable': var,
                    'Category': ref_label,
                    'OR': 1.0,
                    'OR_CI': 'Reference',
                    'P_value': np.nan,
                    'Beta': 0.0,
                    'N': len(y_clean),
                    'Events': y_clean.sum(),
                    'IsReference': True
                })

        # è·å–éå‚è€ƒç±»åˆ«çš„ç»“æœ
        for i, (var_name, category, col_name) in enumerate(var_mapping):
            coef = lr.coef_[0][i]
            or_val = np.exp(coef)

            # Bootstrap CI
            n_bootstrap = 200
            bootstrap_coefs = []

            for _ in range(n_bootstrap):
                idx = np.random.choice(len(y_clean), len(y_clean), replace=True)
                X_boot = X_clean.iloc[idx]
                y_boot = y_clean.iloc[idx]

                try:
                    lr_boot = LogisticRegression(penalty='l2', C=0.5, solver='liblinear',
                                                class_weight='balanced')
                    lr_boot.fit(X_boot, y_boot)
                    bootstrap_coefs.append(lr_boot.coef_[0][i])
                except:
                    continue

            if len(bootstrap_coefs) > 50:
                ci_lower = np.exp(np.percentile(bootstrap_coefs, 2.5))
                ci_upper = np.exp(np.percentile(bootstrap_coefs, 97.5))

                se = np.std(bootstrap_coefs)
                z_score = coef / (se + 1e-8)
                p_val = 2 * (1 - stats.norm.cdf(np.abs(z_score)))

                results.append({
                    'Variable': var_name,
                    'Category': category,
                    'OR': or_val,
                    'OR_CI': f"{or_val:.2f} ({ci_lower:.2f}, {ci_upper:.2f})",
                    'CI_Lower': ci_lower,
                    'CI_Upper': ci_upper,
                    'P_value': p_val,
                    'Beta': coef,
                    'N': len(y_clean),
                    'Events': y_clean.sum(),
                    'IsReference': False
                })

    except Exception as e:
        print(f"Error in multivariate analysis: {e}")

    return pd.DataFrame(results)

# ================== Part 5: ç”Ÿæˆæ ¼å¼åŒ–è¡¨æ ¼ ==================

def format_combined_results_table(uni_results, multi_results, dataset_name):
    """ç”Ÿæˆæ ¼å¼åŒ–çš„ç»“æœè¡¨æ ¼"""

    # å®šä¹‰åˆ†ç±»å˜é‡çš„æ‰€æœ‰ç±»åˆ«ï¼ˆç”¨äºç¡®ä¿å®Œæ•´æ€§ï¼‰
    categorical_vars_structure = {
        'iTED_Risk': ['Low', 'High'],
        'Radiomics_Risk': ['Low', 'High'],
        '3D_ITH_Risk': ['Low', 'High'],
        'Sex': ['Female', 'Male'],
        'Benign_thyroid_lesions': ['No', 'Yes'],
        'Multifocal': ['No', 'Yes'],
        'Infiltrated_the_adjacent_tissue': ['No', 'Yes'],
        'T_stage': ['T1', 'T2', 'T3', 'T4'],
        'N_stage': ['N0', 'N1']
    }

    combined = []

    # è·å–æ‰€æœ‰å˜é‡ï¼ˆä¿æŒé¡ºåºï¼‰
    all_vars_uni = uni_results['Variable'].unique()
    all_vars_multi = multi_results['Variable'].unique()
    all_vars = list(dict.fromkeys(list(all_vars_uni) + list(all_vars_multi)))

    # å®šä¹‰å˜é‡é¡ºåºï¼ˆè¯„åˆ†åœ¨å‰ï¼Œä¸´åºŠå˜é‡åœ¨åï¼‰
    score_vars = ['iTED_Risk', 'Radiomics_Risk', '3D_ITH_Risk']
    clinical_vars = [v for v in all_vars if v not in score_vars]
    ordered_vars = score_vars + clinical_vars

    # æŒ‰å˜é‡å¤„ç†
    for var in ordered_vars:
        if var not in all_vars:
            continue

        # å¦‚æœæ˜¯åˆ†ç±»å˜é‡ï¼Œç¡®ä¿æŒ‰é¢„å®šä¹‰é¡ºåºæ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«
        if var in categorical_vars_structure:
            categories_to_show = categorical_vars_structure[var]

            for category in categories_to_show:
                # è·å–å•å› ç´ ç»“æœ
                uni_row = uni_results[(uni_results['Variable'] == var) &
                                      (uni_results['Category'] == category)]

                # è·å–å¤šå› ç´ ç»“æœ
                multi_row = multi_results[(multi_results['Variable'] == var) &
                                          (multi_results['Category'] == category)]

                result_row = {
                    'Characteristic': var,
                    'Category': category
                }

                # æ·»åŠ å•å› ç´ ç»“æœ
                if not uni_row.empty:
                    uni_row = uni_row.iloc[0]
                    if uni_row.get('IsReference', False):
                        result_row['Univariate_OR'] = 'Reference'
                        result_row['Univariate_P'] = ''
                    else:
                        result_row['Univariate_OR'] = uni_row.get('OR_CI', '')
                        p_val = uni_row.get('P_value', np.nan)
                        if pd.notna(p_val):
                            result_row['Univariate_P'] = f"{p_val:.3f}" if p_val >= 0.001 else "<0.001"
                        else:
                            result_row['Univariate_P'] = ''
                else:
                    result_row['Univariate_OR'] = ''
                    result_row['Univariate_P'] = ''

                # æ·»åŠ å¤šå› ç´ ç»“æœ
                if not multi_row.empty:
                    multi_row = multi_row.iloc[0]
                    if multi_row.get('IsReference', False):
                        result_row['Beta'] = ''
                        result_row['Multivariate_OR'] = 'Reference'
                        result_row['Multivariate_P'] = ''
                    else:
                        beta = multi_row.get('Beta', np.nan)
                        result_row['Beta'] = f"{beta:.3f}" if pd.notna(beta) else ''
                        result_row['Multivariate_OR'] = multi_row.get('OR_CI', '')
                        p_val = multi_row.get('P_value', np.nan)
                        if pd.notna(p_val):
                            result_row['Multivariate_P'] = f"{p_val:.3f}" if p_val >= 0.001 else "<0.001"
                        else:
                            result_row['Multivariate_P'] = ''
                else:
                    result_row['Beta'] = ''
                    result_row['Multivariate_OR'] = ''
                    result_row['Multivariate_P'] = ''

                combined.append(result_row)
        else:
            # è¿ç»­å˜é‡
            # è·å–å•å› ç´ ç»“æœ
            uni_row = uni_results[(uni_results['Variable'] == var) &
                                  (uni_results['Category'] == '')]

            # è·å–å¤šå› ç´ ç»“æœ
            multi_row = multi_results[(multi_results['Variable'] == var) &
                                      (multi_results['Category'] == '')]

            result_row = {
                'Characteristic': var,
                'Category': ''
            }

            # æ·»åŠ å•å› ç´ ç»“æœ
            if not uni_row.empty:
                uni_row = uni_row.iloc[0]
                result_row['Univariate_OR'] = uni_row.get('OR_CI', '')
                p_val = uni_row.get('P_value', np.nan)
                if pd.notna(p_val):
                    result_row['Univariate_P'] = f"{p_val:.3f}" if p_val >= 0.001 else "<0.001"
                else:
                    result_row['Univariate_P'] = ''
            else:
                result_row['Univariate_OR'] = ''
                result_row['Univariate_P'] = ''

            # æ·»åŠ å¤šå› ç´ ç»“æœ
            if not multi_row.empty:
                multi_row = multi_row.iloc[0]
                beta = multi_row.get('Beta', np.nan)
                result_row['Beta'] = f"{beta:.3f}" if pd.notna(beta) else ''
                result_row['Multivariate_OR'] = multi_row.get('OR_CI', '')
                p_val = multi_row.get('P_value', np.nan)
                if pd.notna(p_val):
                    result_row['Multivariate_P'] = f"{p_val:.3f}" if p_val >= 0.001 else "<0.001"
                else:
                    result_row['Multivariate_P'] = ''
            else:
                result_row['Beta'] = ''
                result_row['Multivariate_OR'] = ''
                result_row['Multivariate_P'] = ''

            combined.append(result_row)

    # åˆ›å»ºDataFrame
    df = pd.DataFrame(combined)

    # å»é™¤é‡å¤çš„å˜é‡åï¼ˆä¿æŒç±»åˆ«ç¼©è¿›æ•ˆæœï¼‰
    prev_var = None
    for i, row in df.iterrows():
        if row['Characteristic'] == prev_var:
            df.at[i, 'Characteristic'] = ''
        else:
            prev_var = row['Characteristic']

    return df

# ================== Part 6: ç”Ÿæˆæ£®æ—å›¾ ==================

def create_forest_plot(results_df, title, output_path, analysis_type='univariate'):
    """ç”Ÿæˆé¡¶åˆŠé£æ ¼çš„æ£®æ—å›¾"""

    # ç­›é€‰éå‚è€ƒç±»åˆ«
    plot_data = results_df[~results_df['IsReference']].copy()

    # å¤„ç†ç¼ºå¤±çš„CIå€¼
    if 'CI_Lower' not in plot_data.columns or 'CI_Upper' not in plot_data.columns:
        return

    plot_data = plot_data.dropna(subset=['CI_Lower', 'CI_Upper', 'OR'])

    if plot_data.empty:
        print(f"No data available for forest plot: {title}")
        return

    # åˆ›å»ºæ˜¾ç¤ºæ ‡ç­¾
    plot_data['Display_Label'] = plot_data.apply(
        lambda x: f"{x['Variable']} ({x['Category']})" if x['Category'] else x['Variable'],
        axis=1
    )

    # æŒ‰ORå€¼æ’åº
    plot_data = plot_data.sort_values('OR', ascending=True)

    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(figsize=(10, max(6, len(plot_data) * 0.4)))

    # è®¾ç½®yè½´ä½ç½®
    y_positions = np.arange(len(plot_data))

    # ç»˜åˆ¶ç½®ä¿¡åŒºé—´
    for i, (idx, row) in enumerate(plot_data.iterrows()):
        # ç½®ä¿¡åŒºé—´çº¿
        ax.plot([row['CI_Lower'], row['CI_Upper']], [i, i],
               'k-', linewidth=1.5, alpha=0.7)

        # ORç‚¹
        color = 'red' if row['P_value'] < 0.05 else 'black'
        ax.scatter(row['OR'], i, s=100, c=color, zorder=3, edgecolors='black', linewidth=0.5)

    # æ·»åŠ å‚è€ƒçº¿ï¼ˆOR=1ï¼‰
    ax.axvline(x=1, color='gray', linestyle='--', linewidth=1, alpha=0.5)

    # è®¾ç½®æ ‡ç­¾
    ax.set_yticks(y_positions)
    ax.set_yticklabels(plot_data['Display_Label'])

    # è®¾ç½®xè½´ï¼ˆå¯¹æ•°åˆ»åº¦ï¼‰
    ax.set_xscale('log')
    ax.set_xlabel('Odds Ratio (95% CI)', fontsize=11, fontweight='bold')

    # è®¾ç½®æ ‡é¢˜
    ax.set_title(title, fontsize=12, fontweight='bold', pad=20)

    # è°ƒæ•´å¸ƒå±€
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.5)
    ax.spines['bottom'].set_linewidth(0.5)

    # æ·»åŠ ç½‘æ ¼
    ax.grid(axis='x', alpha=0.2, linestyle='-', linewidth=0.5)
    ax.set_axisbelow(True)

    # æ·»åŠ å›¾ä¾‹
    from matplotlib.lines import Line2D
    legend_elements = [
        Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=8, label='P < 0.05'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor='black', markersize=8, label='P â‰¥ 0.05')
    ]
    ax.legend(handles=legend_elements, loc='upper right', frameon=False)

    # ä¿å­˜å›¾å½¢
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Forest plot saved: {output_path}")

# ================== Part 7: ä¸»æ‰§è¡Œå‡½æ•° ==================

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""

    print("="*80)
    print("COMPREHENSIVE ANALYSIS PIPELINE - PUBLICATION STANDARD")
    print("Strategy: All scores + significant clinical variables (P<0.1)")
    print("Output: Tables and Forest Plots (NEJM/JAMA style)")
    print("="*80)

    # æ•°æ®é›†
    datasets = [('zlyy', 'Training'), ('ydyy', 'Validation')]

    # å­˜å‚¨æ‰€æœ‰ç»“æœç”¨äºæ±‡æ€»
    all_results = []

    for dataset_type, dataset_name in datasets:
        print(f"\n{'='*80}")
        print(f"Processing {dataset_name} Dataset")
        print(f"{'='*80}")

        # 1. åŠ è½½æ•°æ®
        data_info = load_complete_data(dataset_type)
        full_data = data_info['full_data']
        clinical_vars = data_info['clinical_vars']

        print(f"\nClinical variables: {clinical_vars}")

        # 2. åˆ›å»ºæ‰€æœ‰é£é™©åˆ†ç»„ï¼ˆä»é¢„æµ‹æ–‡ä»¶ï¼‰
        print("\n--- Creating risk groups from predictions ---")

        # iTED
        full_data = create_risk_groups_from_predictions(full_data, dataset_type, 'iTED')

        # Radiomics
        full_data = create_risk_groups_from_predictions(full_data, dataset_type, 'Radiomics')

        # 3D_ITH
        full_data = create_risk_groups_from_predictions(full_data, dataset_type, '3D_ITH')

        # 3. æ‰§è¡Œç»¼åˆå•å› ç´ åˆ†æ
        print(f"\n--- Univariate Analysis ---")
        uni_results = perform_comprehensive_univariate_analysis(full_data, clinical_vars)

        if uni_results.empty:
            print(f"No results for {dataset_name}")
            continue

        # æ˜¾ç¤ºå•å› ç´ æ˜¾è‘—çš„å˜é‡
        significant = uni_results[(uni_results['P_value'] < 0.05) & (~uni_results['IsReference'])]
        print(f"\nSignificant variables (P<0.05): {len(significant)}")
        for _, row in significant.head(10).iterrows():
            cat_info = f" ({row['Category']})" if row['Category'] else ""
            print(f"  {row['Variable']}{cat_info}: OR={row.get('OR', np.nan):.2f}, P={row['P_value']:.3f}")

        # 4. æ‰§è¡Œå¤šå› ç´ åˆ†æ
        print(f"\n--- Multivariate Analysis ---")
        multi_results = perform_multivariate_comprehensive(full_data, uni_results)

        # 5. ç”Ÿæˆè¡¨æ ¼
        table = format_combined_results_table(uni_results, multi_results, dataset_name)

        # ä¿å­˜è¡¨æ ¼
        filename = f"Combined_Analysis_{dataset_name}_Comprehensive.csv"
        filepath = os.path.join(OUTPUT_DIR, filename)
        table.to_csv(filepath, index=False)
        print(f"âœ“ Table saved: {filename}")

        # 6. ç”Ÿæˆæ£®æ—å›¾
        # å•å› ç´ æ£®æ—å›¾
        forest_uni_path = os.path.join(OUTPUT_DIR, f"Forest_Univariate_{dataset_name}.png")
        create_forest_plot(uni_results,
                          f"Univariate Analysis - {dataset_name} Dataset",
                          forest_uni_path,
                          'univariate')

        # å¤šå› ç´ æ£®æ—å›¾
        forest_multi_path = os.path.join(OUTPUT_DIR, f"Forest_Multivariate_{dataset_name}.png")
        create_forest_plot(multi_results,
                          f"Multivariate Analysis - {dataset_name} Dataset",
                          forest_multi_path,
                          'multivariate')

        # æ”¶é›†ç»“æœç”¨äºæ±‡æ€»
        all_results.append({
            'dataset': dataset_name,
            'uni_results': uni_results,
            'multi_results': multi_results
        })

    # 7. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*80)
    print("GENERATING SUMMARY REPORT")
    print("="*80)

    summary_data = []

    for result in all_results:
        dataset_name = result['dataset']
        uni_results = result['uni_results']
        multi_results = result['multi_results']

        # æå–ä¸‰ä¸ªè¯„åˆ†çš„ç»“æœ
        for score in ['iTED_Risk', 'Radiomics_Risk', '3D_ITH_Risk']:
            # å•å› ç´ ç»“æœ
            uni_high = uni_results[(uni_results['Variable'] == score) &
                                   (uni_results['Category'] == 'High')]

            # å¤šå› ç´ ç»“æœ
            multi_high = multi_results[(multi_results['Variable'] == score) &
                                      (multi_results['Category'] == 'High')]

            if not uni_high.empty and not multi_high.empty:
                uni_high = uni_high.iloc[0]
                multi_high = multi_high.iloc[0]

                summary_data.append({
                    'Dataset': dataset_name,
                    'Score': score.replace('_Risk', ''),
                    'Univariate_OR': uni_high.get('OR_CI', '-'),
                    'Univariate_P': f"{uni_high['P_value']:.3f}" if uni_high['P_value'] >= 0.001 else "<0.001",
                    'Multivariate_OR': multi_high.get('OR_CI', '-'),
                    'Multivariate_P': f"{multi_high['P_value']:.3f}" if multi_high['P_value'] >= 0.001 else "<0.001"
                })

    # ä¿å­˜æ±‡æ€»
    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(os.path.join(OUTPUT_DIR, 'Summary_Three_Scores.csv'), index=False)
        print("âœ“ Summary report saved")

    print("\n" + "="*80)
    print("ANALYSIS COMPLETED!")
    print("="*80)

    print("\nğŸ“‚ Files generated:")
    print(f"  Location: {OUTPUT_DIR}")
    print("\n  Tables:")
    print("  â€¢ Combined_Analysis_Training_Comprehensive.csv")
    print("  â€¢ Combined_Analysis_Validation_Comprehensive.csv")
    print("\n  Forest Plots:")
    print("  â€¢ Forest_Univariate_Training.png")
    print("  â€¢ Forest_Univariate_Validation.png")
    print("  â€¢ Forest_Multivariate_Training.png")
    print("  â€¢ Forest_Multivariate_Validation.png")
    print("\n  Summary:")
    print("  â€¢ Summary_Three_Scores.csv")

    print("\n" + "="*80)
    print("Publication Ready:")
    print("- Tables show all categories with clear reference groups")
    print("- Forest plots follow NEJM/JAMA style guidelines")
    print("- P<0.05 highlighted in red in forest plots")
    print("="*80)

# è¿è¡Œä¸»ç¨‹åº
if __name__ == "__main__":
    main()
